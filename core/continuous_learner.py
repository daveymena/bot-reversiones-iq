"""
Continuous Learner - Aprende continuamente de operaciones reales
"""
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from trading_gym.trading_env import BinaryOptionsEnv
from core.experience_buffer import ExperienceBuffer

class ContinuousLearner:
    """
    Sistema de aprendizaje continuo que re-entrena el modelo
    con experiencias reales de trading
    """
    def __init__(self, agent, feature_engineer, market_data):
        self.agent = agent
        self.feature_engineer = feature_engineer
        self.market_data = market_data
        self.experience_buffer = ExperienceBuffer()
        
        # Configuraci√≥n de Re-entrenamiento
        self.min_experiences_to_train = 20  # M√≠nimo de experiencias (reducido de 50 a 20)
        self.retrain_frequency = 20  # Re-entrenar cada 20 operaciones (reducido de 100 a 20)
        self.retrain_timesteps = 2000  # Pasos de re-entrenamiento
        
        # Configuraci√≥n de Evaluaci√≥n Continua
        self.evaluation_frequency = 10  # Evaluar cada 10 operaciones
        self.min_win_rate = 0.40  # Win rate m√≠nimo aceptable (40%)
        self.max_consecutive_losses = 5  # M√°ximo de p√©rdidas consecutivas antes de re-entrenar
        
        # Control de re-entrenamientos
        # IMPORTANTE: Inicializar con el n√∫mero de experiencias cargadas para evitar bucle
        self.last_retrain_count = len(self.experience_buffer.experiences)
        self.retraining_in_progress = False  # Flag para evitar re-entrenamientos simult√°neos
        self.last_retrain_time = 0  # Timestamp del √∫ltimo re-entrenamiento
        self.retrain_cooldown = 300  # Cooldown de 5 minutos (300s) despu√©s de re-entrenar
        
    def add_real_trade_experience(self, state_before, action, profit, state_after, metadata=None):
        """
        Agrega experiencia de una operaci√≥n real
        
        Args:
            state_before: DataFrame con indicadores antes de la operaci√≥n
            action: Acci√≥n tomada (0=HOLD, 1=CALL, 2=PUT)
            profit: Resultado en $ (positivo=ganancia, negativo=p√©rdida)
            state_after: DataFrame con indicadores despu√©s
            metadata: Info adicional (activo, timestamp, etc.)
        """
        # Convertir DataFrames a arrays
        if isinstance(state_before, pd.DataFrame):
            state_before = state_before.values.flatten()
        if isinstance(state_after, pd.DataFrame):
            state_after = state_after.values.flatten()
        
        # Normalizar reward (profit)
        # Convertir $ a reward normalizado
        reward = profit  # Mantener el valor real
        
        # Determinar si termin√≥ (siempre False para trading continuo)
        done = False
        
        # Agregar al buffer
        self.experience_buffer.add_experience(
            state=state_before,
            action=action,
            reward=reward,
            next_state=state_after,
            done=done,
            metadata=metadata
        )
        
        print(f"üìù Experiencia agregada: Action={action}, Reward=${profit:.2f}")
        
        # EVALUACI√ìN CONTINUA cada N experiencias
        total_exp = len(self.experience_buffer.experiences)
        
        # Evitar re-entrenamientos si ya se hizo uno recientemente
        if self.retraining_in_progress:
            return
        
        # Solo evaluar si hay experiencias NUEVAS desde el √∫ltimo re-entrenamiento
        new_experiences = total_exp - self.last_retrain_count
        
        # Evaluar cada 10 operaciones NUEVAS
        if new_experiences >= self.evaluation_frequency and new_experiences % self.evaluation_frequency == 0:
            print(f"\nüìä EVALUACI√ìN CONTINUA (Operaci√≥n #{total_exp}, {new_experiences} nuevas)")
            evaluation = self.evaluate_performance()
            print(f"   {evaluation['reason']}")
            
            if evaluation['should_retrain']:
                print(f"   üéì Acci√≥n: {evaluation['action']}")
                if evaluation['action'] == 'RETRAIN_URGENT':
                    print(f"   ‚ö†Ô∏è RE-ENTRENAMIENTO URGENTE")
                self.retrain_from_experiences()
        
        # Re-entrenar cada N experiencias NUEVAS (frecuencia normal)
        elif new_experiences >= self.retrain_frequency:
            print(f"\nüéì Re-entrenamiento programado ({new_experiences} experiencias nuevas)")
            self.retrain_from_experiences()
    
    def retrain_from_experiences(self):
        """
        Re-entrena el modelo usando experiencias reales + datos frescos
        """
        # Evitar re-entrenamientos simult√°neos
        if self.retraining_in_progress:
            print("‚ö†Ô∏è Re-entrenamiento ya en progreso, saltando...")
            return False

        try:
            self.retraining_in_progress = True

            # Obtener experiencias recientes
            experiences = self.experience_buffer.get_recent_experiences(500)

            if len(experiences) < self.min_experiences_to_train:
                print(f"‚ö†Ô∏è Pocas experiencias ({len(experiences)}), se necesitan al menos {self.min_experiences_to_train}")
                return False

            print(f"üìä Preparando {len(experiences)} experiencias para entrenamiento...")

            # Estad√≠sticas ANTES del re-entrenamiento
            stats = self.experience_buffer.get_statistics()
            print(f"\nüìä Estad√≠sticas ANTES del re-entrenamiento:")
            print(f"   Total: {stats['total']}")
            print(f"   Ganadas: {stats['wins']}")
            print(f"   Perdidas: {stats['losses']}")
            print(f"   Win Rate: {stats['win_rate']:.1f}%")
            print(f"   Profit Total: ${stats['total_profit']:.2f}")

            # Evaluar si necesita re-entrenamiento
            evaluation = self.evaluate_performance()
            if not evaluation['should_retrain']:
                print(f"\n‚úÖ Rendimiento aceptable, no se necesita re-entrenamiento")
                self.last_retrain_count = len(self.experience_buffer.experiences)
                return True

            # Intentar re-entrenamiento inteligente basado en experiencias
            print(f"\nüß† Intentando re-entrenamiento inteligente...")

            # Si hay suficientes experiencias de calidad, entrenar con ellas
            if len(experiences) >= 100:
                success = self._train_on_experiences(experiences)
                if success:
                    print("‚úÖ Re-entrenamiento con experiencias exitoso")
                    self.last_retrain_count = len(self.experience_buffer.experiences)
                    return True

            # Fallback: re-entrenar con datos frescos
            print(f"‚ö†Ô∏è Re-entrenamiento con experiencias fall√≥ o insuficiente, usando datos frescos...")
            result = self.retrain_with_fresh_data()

            # Actualizar contador de √∫ltimo re-entrenamiento
            self.last_retrain_count = len(self.experience_buffer.experiences)
            
            # IMPORTANTE: Actualizar timestamp del √∫ltimo re-entrenamiento
            import time
            self.last_retrain_time = time.time()
            
            return result

        except Exception as e:
            print(f"‚ùå Error en re-entrenamiento: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            # IMPORTANTE: Siempre resetear el flag, incluso si hay error
            self.retraining_in_progress = False
    
    def retrain_with_fresh_data(self, asset="EURUSD-OTC", num_candles=1000):
        """
        Re-entrena con datos frescos del broker
        Combina datos hist√≥ricos con experiencias reales
        """
        print(f"\nüîÑ Re-entrenando con datos frescos de {asset}...")
        
        try:
            # Obtener datos frescos
            import time
            df = self.market_data.get_candles(asset, 60, num_candles, time.time())
            
            if df.empty:
                print("‚ùå No se pudieron obtener datos")
                return False
            
            print(f"‚úÖ Obtenidas {len(df)} velas")
            
            # Procesar indicadores
            df_processed = self.feature_engineer.prepare_for_rl(df)
            
            if df_processed.empty:
                print("‚ùå Error procesando indicadores")
                return False

            print(f"‚úÖ Indicadores calculados ({df_processed.shape[1]} features)")
            
            # Crear entorno
            env = DummyVecEnv([lambda: BinaryOptionsEnv(
                data=df_processed,
                feature_engineer=self.feature_engineer
            )])
            
            # Re-entrenar con timeout de seguridad
            print(f"üéì Re-entrenando por {self.retrain_timesteps} pasos (con timeout)...")

            # Actualizar entorno del agente
            self.agent.env = env

            # Timeout m√°s corto para re-entrenamiento (2 minutos m√°ximo)
            timeout_seconds = 120

            # Si el modelo existe, continuar entrenamiento
            if self.agent.model is not None:
                print("üì¶ Actualizando modelo existente...")
                self.agent.model.set_env(env)
                # Entrenar directamente
                success = self.agent.train(timesteps=self.retrain_timesteps, timeout_seconds=timeout_seconds)
            else:
                print("üì¶ Creando nuevo modelo...")
                # Crear nuevo modelo
                success = self.agent.train(timesteps=self.retrain_timesteps, timeout_seconds=timeout_seconds)

            if not success:
                print("‚ö†Ô∏è Re-entrenamiento fall√≥, pero continuando con modelo anterior...")
                return False
            
            # Guardar
            print("üíæ Guardando modelo...")
            self.agent.save()
            
            print("‚úÖ Re-entrenamiento completado exitosamente")
            
            # Mostrar estad√≠sticas de experiencias
            stats = self.experience_buffer.get_statistics()
            print(f"\nüìä Experiencias acumuladas:")
            print(f"   Total: {stats['total']}")
            print(f"   Win Rate: {stats['win_rate']:.1f}%")
            print(f"   Profit Total: ${stats['total_profit']:.2f}")
            
            # IMPORTANTE: Actualizar contador para evitar bucle
            self.last_retrain_count = len(self.experience_buffer.experiences)
            
            # IMPORTANTE: Actualizar timestamp del √∫ltimo re-entrenamiento
            import time
            self.last_retrain_time = time.time()
            
            print(f"üîÑ Continuando operaciones normales...")
            print(f"‚è≥ Cooldown de {self.retrain_cooldown}s activado para evitar bucle\n")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error en re-entrenamiento: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_learning_stats(self):
        """Obtiene estad√≠sticas del aprendizaje"""
        return self.experience_buffer.get_statistics()
    
    def evaluate_performance(self):
        """
        Eval√∫a el rendimiento actual del bot
        
        Returns:
            dict: {
                'should_retrain': bool,
                'reason': str,
                'stats': dict,
                'action': str
            }
        """
        stats = self.experience_buffer.get_statistics()
        
        result = {
            'should_retrain': False,
            'reason': '',
            'stats': stats,
            'action': 'CONTINUE'
        }
        
        # No hay suficientes datos
        if stats['total'] < 10:
            result['reason'] = "Pocas operaciones para evaluar"
            return result
        
        # Obtener √∫ltimas 10 experiencias
        recent = self.experience_buffer.get_recent_experiences(10)
        recent_wins = sum(1 for exp in recent if exp['reward'] > 0)
        recent_win_rate = (recent_wins / len(recent)) * 100 if recent else 0
        
        # Calcular p√©rdidas consecutivas
        consecutive_losses = 0
        for exp in reversed(recent):
            if exp['reward'] < 0:
                consecutive_losses += 1
            else:
                break
        
        # CRITERIO 1: Win rate muy bajo (m√°s estricto)
        if recent_win_rate < 35:  # Reducido de 40% a 35%
            result['should_retrain'] = True
            result['reason'] = f"Win rate cr√≠tico ({recent_win_rate:.0f}% < 35%)"
            result['action'] = 'RETRAIN_URGENT'
            return result

        # CRITERIO 2: Muchas p√©rdidas consecutivas (m√°s sensible)
        if consecutive_losses >= 4:  # Reducido de 5 a 4
            result['should_retrain'] = True
            result['reason'] = f"{consecutive_losses} p√©rdidas consecutivas"
            result['action'] = 'RETRAIN_URGENT'
            return result

        # CRITERIO 3: Profit negativo significativo
        recent_profit = sum(exp['reward'] for exp in recent)
        if recent_profit < -30:  # Reducido de -50 a -30
            result['should_retrain'] = True
            result['reason'] = f"Profit negativo reciente (${recent_profit:.2f})"
            result['action'] = 'RETRAIN'
            return result

        # CRITERIO 4: Tendencia negativa (nuevo)
        if len(recent) >= 15:
            # Calcular win rate en √∫ltimas 5 vs primeras 10
            first_10 = recent[:10]
            last_5 = recent[-5:]
            first_win_rate = (sum(1 for exp in first_10 if exp['reward'] > 0) / len(first_10)) * 100
            last_win_rate = (sum(1 for exp in last_5 if exp['reward'] > 0) / len(last_5)) * 100

            if last_win_rate < first_win_rate - 15:  # Ca√≠da de al menos 15%
                result['should_retrain'] = True
                result['reason'] = f"Tendencia negativa (Win rate: {first_win_rate:.0f}% ‚Üí {last_win_rate:.0f}%)"
                result['action'] = 'RETRAIN'
                return result
        
        # Todo bien
        result['reason'] = f"Rendimiento aceptable (Win rate: {recent_win_rate:.0f}%)"
        result['action'] = 'CONTINUE'
        return result
    
    def should_pause_trading(self):
        """
        Determina si el bot deber√≠a pausar operaciones
        
        Returns:
            tuple: (should_pause: bool, reason: str)
        """
        import time
        
        # COOLDOWN: No pausar si acabamos de re-entrenar recientemente
        time_since_retrain = time.time() - self.last_retrain_time
        if time_since_retrain < self.retrain_cooldown:
            remaining = int(self.retrain_cooldown - time_since_retrain)
            return False, f"‚è≥ Cooldown post-entrenamiento: {remaining}s restantes"
        
        stats = self.experience_buffer.get_statistics()
        
        # No hay suficientes datos
        if stats['total'] < 5:
            return False, ""
        
        # Obtener √∫ltimas experiencias
        recent = self.experience_buffer.get_recent_experiences(10)
        
        # Calcular p√©rdidas consecutivas
        consecutive_losses = 0
        for exp in reversed(recent):
            if exp['reward'] < 0:
                consecutive_losses += 1
            else:
                break
        
        # PAUSAR si hay muchas p√©rdidas consecutivas
        if consecutive_losses >= self.max_consecutive_losses:
            return True, f"üõë {consecutive_losses} p√©rdidas consecutivas - PAUSANDO para re-entrenar"
        
        # PAUSAR si el win rate es muy bajo
        if len(recent) >= 10:
            recent_wins = sum(1 for exp in recent if exp['reward'] > 0)
            recent_win_rate = (recent_wins / len(recent)) * 100
            
            if recent_win_rate < 30:  # Menos del 30%
                return True, f"üõë Win rate cr√≠tico ({recent_win_rate:.0f}%) - PAUSANDO para re-entrenar"
        
        return False, ""
