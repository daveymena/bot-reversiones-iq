#!/usr/bin/env python3
"""
Test para verificar que ambas IAs (Groq + Ollama) funcionan correctamente
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ai.llm_client import LLMClient
from config import Config

def test_groq_connection():
    """Prueba la conexi√≥n a Groq"""
    print("üß† PROBANDO GROQ...")
    print(f"   Llaves disponibles: {len(Config.GROQ_API_KEYS)}")
    print(f"   USE_GROQ: {Config.USE_GROQ}")
    
    if not Config.GROQ_API_KEYS:
        print("‚ùå No hay llaves de Groq configuradas")
        return False
    
    try:
        llm = LLMClient()
        if llm.use_groq and llm.groq_client:
            # Hacer una consulta simple
            response = llm._safe_query("¬øFunciona Groq? Responde solo 'S√ç' o 'NO'")
            
            if "Error" not in response and len(response) > 0:
                print(f"‚úÖ Groq funciona: {response[:50]}...")
                return True
            else:
                print(f"‚ùå Groq no responde correctamente: {response}")
                return False
        else:
            print("‚ùå Cliente Groq no inicializado")
            return False
    except Exception as e:
        print(f"‚ùå Error probando Groq: {e}")
        return False

def test_ollama_connection():
    """Prueba la conexi√≥n a Ollama"""
    print("\nü§ñ PROBANDO OLLAMA...")
    print(f"   URL: {Config.OLLAMA_BASE_URL}")
    print(f"   Modelo: {Config.OLLAMA_MODEL}")
    
    try:
        llm = LLMClient()
        # Forzar uso de Ollama
        response = llm._query_ollama("¬øFunciona Ollama? Responde solo 'S√ç' o 'NO'")
        
        if "Error" not in response and len(response) > 0:
            print(f"‚úÖ Ollama funciona: {response[:50]}...")
            return True
        else:
            print(f"‚ùå Ollama no responde correctamente: {response}")
            return False
    except Exception as e:
        print(f"‚ùå Error probando Ollama: {e}")
        return False

def test_fallback_system():
    """Prueba el sistema de fallback Groq -> Ollama"""
    print("\nüîÑ PROBANDO SISTEMA DE FALLBACK...")
    
    try:
        llm = LLMClient()
        
        # Simular agotamiento de Groq
        original_use_groq = llm.use_groq
        llm.use_groq = False  # Forzar uso de Ollama
        
        response = llm._safe_query("Test de fallback. Responde 'OLLAMA ACTIVO'")
        
        if "OLLAMA" in response.upper() or len(response) > 5:
            print("‚úÖ Sistema de fallback funciona correctamente")
            
            # Restaurar configuraci√≥n
            llm.use_groq = original_use_groq
            return True
        else:
            print(f"‚ùå Fallback no funciona: {response}")
            return False
    except Exception as e:
        print(f"‚ùå Error probando fallback: {e}")
        return False

def test_trading_decision():
    """Prueba una decisi√≥n de trading completa"""
    print("\nüìä PROBANDO DECISI√ìN DE TRADING...")
    
    try:
        llm = LLMClient()
        
        # Datos de prueba
        market_summary = "EURUSD: 1.1740 | RSI: 45 | MACD: Neutral"
        smart_money = "Setup: TREND_PULLBACK_CALL | Score: 75%"
        learning = "Performance reciente: 60%"
        
        decision = llm.analyze_complete_trading_opportunity(
            market_data_summary=market_summary,
            smart_money_analysis=smart_money,
            learning_insights=learning,
            asset="EURUSD-OTC",
            current_balance=1000
        )
        
        if decision and 'should_trade' in decision:
            print(f"‚úÖ Decisi√≥n generada:")
            print(f"   Should trade: {decision.get('should_trade')}")
            print(f"   Direction: {decision.get('direction')}")
            print(f"   Confidence: {decision.get('confidence')}")
            print(f"   Reason: {decision.get('primary_reason', 'N/A')}")
            return True
        else:
            print(f"‚ùå Decisi√≥n inv√°lida: {decision}")
            return False
    except Exception as e:
        print(f"‚ùå Error probando decisi√≥n: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Funci√≥n principal"""
    print("üî¨ TEST DE SISTEMA DUAL AI (GROQ + OLLAMA)")
    print("=" * 60)
    
    results = []
    
    # Test 1: Groq
    results.append(("Groq Connection", test_groq_connection()))
    
    # Test 2: Ollama
    results.append(("Ollama Connection", test_ollama_connection()))
    
    # Test 3: Fallback
    results.append(("Fallback System", test_fallback_system()))
    
    # Test 4: Trading Decision
    results.append(("Trading Decision", test_trading_decision()))
    
    # Resumen
    print("\n" + "=" * 60)
    print("üìã RESUMEN DE PRUEBAS")
    print("=" * 60)
    
    passed = 0
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{test_name:20} {status}")
        if result:
            passed += 1
    
    print(f"\nResultado: {passed}/{len(results)} pruebas exitosas")
    
    if passed == len(results):
        print("üéâ SISTEMA DUAL AI FUNCIONANDO CORRECTAMENTE")
        print("\nConfiguraci√≥n recomendada:")
        print("- Groq como IA principal (r√°pida)")
        print("- Ollama como respaldo (cuando Groq falla)")
        print("- Fallback autom√°tico sin intervenci√≥n")
    else:
        print("‚ö†Ô∏è ALGUNOS COMPONENTES NECESITAN ATENCI√ìN")
        
        if not results[0][1]:  # Groq fall√≥
            print("\nüîß SOLUCIONES PARA GROQ:")
            print("- Verificar llaves API en .env")
            print("- Comprobar l√≠mites de uso")
            print("- Verificar conexi√≥n a internet")
        
        if not results[1][1]:  # Ollama fall√≥
            print("\nüîß SOLUCIONES PARA OLLAMA:")
            print("- Verificar que EasyPanel est√© activo")
            print("- Comprobar URL de Ollama")
            print("- Cambiar modelo a llama3.2:3b")
    
    return passed == len(results)

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)